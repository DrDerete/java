Сначала нужно описать среду, в которой действует агент. Это включает в себя состояния среды, например, координаты робота в лабиринте, действия агента, такие как движение вверх, вниз, влево, вправо, вероятности переходов, например, с вероятностью 0.8 робот движется в выбранном направлении, а с вероятностью 0.2 — в случайном соседнем направлении, и награды, например, +10 за достижение выхода, -1 за каждое движение.

Для каждого состояния вычисляется ожидаемая награда, которую агент получит, начиная с этого состояния и следуя определённой стратегии. Это называется функцией ценности. Используя функцию ценности, можно определить, какое действие лучше всего выбрать в каждом состоянии, чтобы максимизировать ожидаемую награду.


Решение с помощью Value Iteration начинается с инициализации функции ценности для всех состояний нулями. Для каждого состояния обновляем значение функции ценности, используя формулу, которая учитывает награду за действие, вероятность перехода в новое состояние и значение функции ценности для этого нового состояния. Процесс повторяется до тех пор, пока изменения в функции ценности не станут меньше определённого порога. Оптимальная политика выбирает действие, которое максимизирует ожидаемую награду.